{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "# If any of these don't work, try doing `pip install _____`, or try looking up the error message.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os.path\n",
    "from os import path\n",
    "import math\n",
    "import datetime\n",
    "import unidecode\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules from sportsrefernece.ncaab for college basketball\n",
    "from sportsreference.ncaab.boxscore import Boxscore as NCAAB_Boxscore\n",
    "from sportsreference.ncaab.conferences import Conferences as NCAAB_Conferences\n",
    "from sportsreference.ncaab.rankings import Rankings as NCAAB_Rankings\n",
    "from sportsreference.ncaab.roster import Player as NCAAB_Player\n",
    "from sportsreference.ncaab.roster import Roster as NCAAB_Roster\n",
    "from sportsreference.ncaab.schedule import Schedule as NCAAB_Schedule\n",
    "from sportsreference.ncaab.teams import Teams as NCAAB_Teams\n",
    "\n",
    "# Modules from sportsrefernece.nba for NBA basketball\n",
    "from sportsreference.nba.boxscore import Boxscore as NBA_Boxscore\n",
    "from sportsreference.nba.roster import Player as NBA_Player\n",
    "from sportsreference.nba.roster import Roster as NBA_Roster\n",
    "from sportsreference.nba.schedule import Schedule as NBA_Schedule\n",
    "from sportsreference.nba.teams import Teams as NBA_Teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi everyone! In this notebook, we will be constructing a variety of different machine learning models to predict NBA rookie statlines from different college players input from the Anvil App!\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. College Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order for us to pass up college stats for this player to be viewed in the front-end, we will need to:**\n",
    "1. Clean the name\n",
    "2. Find the college player stats for the input name\n",
    "    - If they don't exist, tell the user that they don't, and provide the correct error message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find college player stats for input name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method should hopefully reduce the number of failure cases.\n",
    "def convert_nba_ncaa_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts the format of the NBA player_id to the NCAA player_id.\n",
    "\n",
    "    You may want to elaborate on the logic on this function to reduce the number of failure cases later.\n",
    "    \"\"\"\n",
    "    return unidecode.unidecode(name.lower().replace(\" \", \"-\") + \"-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find the college player stats for the input name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_college_stats(player_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Return a dictionary with the following keys:\n",
    "    \n",
    "    success: If the player was correctly found (boolean)\n",
    "    data: Player data (pd.Dataframe)\n",
    "    error: Error message, if success is false (string)\n",
    "    \"\"\"\n",
    "    response = {}\n",
    "    clean_name = convert_nba_ncaa_name(player_name)\n",
    "    \n",
    "    # Checking if player does exist with current name\n",
    "    try:\n",
    "        player_data = NCAAB_Player(clean_name).dataframe\n",
    "    except TypeError:\n",
    "        response['success'] = False\n",
    "        response['data'] = None\n",
    "        response['error'] = \"This player doesn't exist (or its name is not in the correct format). Please try a different player name.\"\n",
    "        return response\n",
    "\n",
    "    player_data.rename(columns=lambda x: 'NCAAB_' + x, inplace=True)\n",
    "    last_year = player_data.iloc[[player_data.shape[0] - 2]]\n",
    "    last_year.set_index(pd.Index(data=[player_name], name='Name'), inplace=True)\n",
    "    \n",
    "    response['success'] = True\n",
    "    response['data'] = last_year # Change for Matt's formatting\n",
    "    response['error'] = None\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ayton = get_college_stats('Deandre Ayton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ayton['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pip install the library if you don't currently have it\n",
    "!pip install anvil-uplink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making front-end endpoint for retrieving college data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.server\n",
    "anvil.server.connect(\"INSERT THE KEY FROM YOUR ANVIL APP HERE\")\n",
    "\n",
    "@anvil.server.callable\n",
    "def get_college(name):\n",
    "    \"\"\"\n",
    "    Making the front-end to back-end connection, and passing up college data.\n",
    "    \"\"\"\n",
    "    response = get_college_stats(name)\n",
    "    if not response['success']:\n",
    "        return response\n",
    "    \n",
    "    column_names = ['NCAAB_' + name for name in ['points', 'total_rebounds', 'assists', 'steals', 'blocks', 'games_played']]\n",
    "    \n",
    "    def rename_cols(name):\n",
    "        \"\"\"\n",
    "        Rename columns\n",
    "        \"\"\"\n",
    "        new_names = {\n",
    "            'NCAAB_points': 'pts',\n",
    "            'NCAAB_total_rebounds': 'reb',\n",
    "            'NCAAB_assists': 'ast',\n",
    "            'NCAAB_steals': 'stl',\n",
    "            'NCAAB_blocks': 'blk',\n",
    "        }\n",
    "        if name == 'Name':\n",
    "            return name\n",
    "        return new_names[name]\n",
    "    \n",
    "    new_data = response['data']\n",
    "    per_game = (new_data[column_names] / int(new_data['NCAAB_games_played'].values)).drop(columns=['NCAAB_games_played'])\n",
    "    renamed = per_game.iloc[0].apply(lambda x: round(x, 2)).rename(lambda x: rename_cols(x))\n",
    "    renamed['Name'] = name\n",
    "    response['data'] = renamed.to_dict()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_college('Ben Simmons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_college('Stephen Curry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Clean/Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('data/player_data_final.csv')\n",
    "NBA_col = ['NBA_points', 'NBA_total_rebounds', 'NBA_assists', 'NBA_steals', 'NBA_blocks']\n",
    "train, test = data.drop(columns=NBA_col), data[NBA_col].set_index(pd.Index(data=data['name'], name='name'))\n",
    "train.set_index('name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null(df):\n",
    "    \"\"\"\n",
    "    Returns all columns that have a null value in the player's data\n",
    "    \"\"\"\n",
    "    return df.columns[df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order for us to make predictions from the college data to be rendering in the front-end, we first need to:**\n",
    "1. Drop unneeded columns\n",
    "2. Clean the data the same way we did in our initial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drops the columns from the input college data.\n",
    "    \"\"\"\n",
    "    def drop_for_nans(data):\n",
    "        # Drops columns due to NaNs inside the data.\n",
    "        drop_col = [\n",
    "            'NCAAB_box_plus_minus',\n",
    "            'NCAAB_defensive_box_plus_minus',\n",
    "            'NCAAB_offensive_box_plus_minus',\n",
    "            'NCAAB_player_efficiency_rating',\n",
    "            'NCAAB_three_point_percentage',\n",
    "        ]\n",
    "        return data.drop(columns=drop_col)\n",
    "    \n",
    "    def drop_for_qual(data):\n",
    "        # Drops columns due to qualitative data\n",
    "        qual_drop = [\n",
    "            'NCAAB_conference', \n",
    "            'NCAAB_player_id',\n",
    "            'NCAAB_team_abbreviation',\n",
    "        ]\n",
    "        return data.drop(columns=qual_drop)\n",
    "    \n",
    "    return drop_for_nans(drop_for_qual(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curry = get_college_stats('Stephen Curry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_curry = drop_columns(curry['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null(new_curry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clean the data the same way we did initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_qualitative(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the qualitative columns (height, position)\n",
    "    \"\"\"\n",
    "\n",
    "    def convert_height(height: str) -> int:\n",
    "        \"\"\"\n",
    "        Convert height from string to int (6-11 -> 83)\n",
    "        \"\"\"\n",
    "        feet, inches = height.split(\"-\")\n",
    "        return int(feet) * 12 + int(inches)\n",
    "\n",
    "    def clean_position(position: str) -> str:\n",
    "        \"\"\"\n",
    "        If the player has a hypened position, remove the second one.\n",
    "        \"\"\"\n",
    "        return position.split('-')[0]\n",
    "    \n",
    "    def one_hot_position(player):\n",
    "        \"\"\"\n",
    "        Applies one hot encoding to the player's position.\n",
    "        \"\"\"\n",
    "        positions = positions = ['Center', 'Forward', 'Guard']\n",
    "        player[positions] = pd.DataFrame([\n",
    "            [int(p == player['NCAAB_position']) for p in positions]\n",
    "        ], index=player.index)\n",
    "        return player\n",
    "    \n",
    "    data['NCAAB_height'] = data['NCAAB_height'].apply(convert_height)\n",
    "    data['NCAAB_position'] = data['NCAAB_position'].apply(clean_position)\n",
    "    return one_hot_position(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_curry = clean_qualitative(new_curry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null(clean_curry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes the columns we've qualitative selected from the dataset to be our features for our model.\n",
    "    \"\"\"\n",
    "    columns_to_keep = [\n",
    "        'NCAAB_assists',\n",
    "        'NCAAB_blocks',\n",
    "        'NCAAB_field_goal_attempts',\n",
    "        'NCAAB_field_goal_percentage',\n",
    "        'NCAAB_field_goals',\n",
    "        'NCAAB_free_throw_attempt_rate',\n",
    "        'NCAAB_free_throw_attempts',\n",
    "        'NCAAB_free_throw_percentage',\n",
    "        'NCAAB_free_throws',\n",
    "        'NCAAB_games_played',\n",
    "        'NCAAB_games_started',\n",
    "        'NCAAB_height',\n",
    "        'NCAAB_personal_fouls',\n",
    "        'NCAAB_points',\n",
    "        'NCAAB_steal_percentage',\n",
    "        'NCAAB_steals',\n",
    "        'NCAAB_three_point_attempt_rate',\n",
    "        'NCAAB_three_point_attempts',\n",
    "        'NCAAB_total_rebound_percentage',\n",
    "        'NCAAB_total_rebounds',\n",
    "        'NCAAB_turnover_percentage',\n",
    "        'NCAAB_turnovers',\n",
    "        'NCAAB_two_point_attempts',\n",
    "        'NCAAB_two_point_percentage',\n",
    "        'NCAAB_win_shares',\n",
    "        'Guard',\n",
    "        'Forward',\n",
    "        'Center',\n",
    "    ]\n",
    "    return data[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_curry = feature_extraction(clean_curry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null(final_curry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_cleaning(player_data, train=False):\n",
    "    \"\"\"\n",
    "    Cleans the player's data further if there's any NaN values (older players).\n",
    "    \"\"\"\n",
    "    def check_null(df):\n",
    "        \"\"\"\n",
    "        Returns all columns that have a null value in the player's data\n",
    "        \"\"\"\n",
    "        return df.columns[df.isna().any()].tolist()\n",
    "    \n",
    "    columns = check_null(player_data)\n",
    "    if columns:\n",
    "        for column in columns:\n",
    "            # Replace value with the average of our training set\n",
    "            player_data.loc[:, column] = train.mean()[column]\n",
    "    return player_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(player_data, train):\n",
    "    \"\"\"\n",
    "    Returns the processed data in correct format for inputting into our models.\n",
    "    \"\"\"\n",
    "    cols_dropped = drop_columns(player_data)\n",
    "    clean_data = clean_qualitative(cols_dropped)\n",
    "    featured_data = feature_extraction(clean_data)\n",
    "    final_data = extra_cleaning(featured_data, train)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_data(curry['data'], train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Make Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order for us to make our predictions, we first take a look at all the different models we've made for the project.**\n",
    "\n",
    "1. LinearRegression\n",
    "2. K-Means\n",
    "3. DecisionTrees\n",
    "4. Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, test, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-shuffle data\n",
    "def shuffle(train, test):\n",
    "    \"\"\"\n",
    "    Shuffe the data around for re-trying results.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, test, test_size=0.2)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def RMSE(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the root mean squared error of the model's predictions.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((y - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total loss function\n",
    "def all_RMSE(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the RMSE by each of the different stats, and the total RMSE\n",
    "    \"\"\"\n",
    "    rmses = {}\n",
    "    for num, name in enumerate(y_pred.columns):\n",
    "        rmses[name] = RMSE(y[:, num], y_pred[name])\n",
    "    rmses['Total RMSE'] = sum(rmses.values())\n",
    "    return pd.Series(rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize our data\n",
    "def standardize(train, test):\n",
    "    \"\"\"\n",
    "    Standardize both train and test based off of train's means and std's.\n",
    "    \"\"\"\n",
    "    means, stds = train.mean(), train.std()\n",
    "    norm_train = (train - means) / stds\n",
    "    norm_test = (test - means) / stds\n",
    "    return norm_train, norm_test, means, stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1A. Linear Regression (without standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = shuffle(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = linear_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE for points\n",
    "RMSE(y_test.values[:, 0], y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Total RMSE\n",
    "all_RMSE(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B. Linear Regression (with standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = shuffle(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "norm_X_train, norm_X_test, _, _ = standardize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(norm_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = linear_model.predict(norm_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE for points\n",
    "RMSE(y_test.values[:, 0], y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total RMSE\n",
    "all_RMSE(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data up\n",
    "X_train, X_test, y_train, y_test = shuffle(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "norm_X_train, norm_X_test, _, _ = standardize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and produce labels\n",
    "from sklearn.cluster import KMeans\n",
    "k_means_cluster = KMeans(n_clusters=120, max_iter=10000, n_init=100).fit(norm_X_train.values)\n",
    "X_copy = norm_X_train.copy(deep=True)\n",
    "X_copy['label'] = k_means_cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the predicted labels of the test set\n",
    "label_pred = k_means_cluster.predict(norm_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_predictions(labels, X_copy, y_train):\n",
    "    \"\"\"\n",
    "    Finds the predictions for these different players\n",
    "    \"\"\"\n",
    "    all_preds = []\n",
    "    for label in label_pred:\n",
    "        similar_players = X_copy[X_copy['label'] == label].index\n",
    "        similar_avg_stats = y_train.loc[similar_players].mean()\n",
    "        all_preds.append(similar_avg_stats.values)\n",
    "    return np.array(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find stat predictions for the test set\n",
    "y_pred = find_predictions(label_pred, X_copy, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE for points\n",
    "RMSE(y_test.values[:, 0], y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Total RMSE\n",
    "all_RMSE(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B. Run K-Means multiple times for long-term RMSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_x_times(x):\n",
    "    \"\"\"\n",
    "    Runs K-Means 'x' number of times to average the RMSEs.\n",
    "    \"\"\"\n",
    "    total_avg = []\n",
    "    for _ in range(10):\n",
    "        # Shuffle the data up\n",
    "        X_train, X_test, y_train, y_test = shuffle(train, test)\n",
    "\n",
    "        # Standardize data\n",
    "        norm_X_train, norm_X_test, _, _ = standardize(X_train, X_test)\n",
    "\n",
    "        # Fit model and produce labels\n",
    "        from sklearn.cluster import KMeans\n",
    "        k_means_cluster = KMeans(n_clusters=120, max_iter=10000, n_init=100).fit(norm_X_train.values)\n",
    "        X_copy = norm_X_train.copy(deep=True)\n",
    "        X_copy['label'] = k_means_cluster.labels_\n",
    "\n",
    "        # Find the predicted labels of the test set\n",
    "        label_pred = k_means_cluster.predict(norm_X_test)\n",
    "\n",
    "        def find_predictions(labels, X_copy, y_train):\n",
    "            \"\"\"\n",
    "            Finds the predictions for these different players\n",
    "            \"\"\"\n",
    "            all_preds = []\n",
    "            for label in label_pred:\n",
    "                similar_players = X_copy[X_copy['label'] == label].index\n",
    "                similar_avg_stats = y_train.loc[similar_players].mean()\n",
    "                all_preds.append(similar_avg_stats.values)\n",
    "            return np.array(all_preds)\n",
    "\n",
    "        # Find stat predictions for the test set\n",
    "        y_pred = find_predictions(label_pred, X_copy, y_train)\n",
    "        total_avg.append(all_RMSE(y_pred, y_test).values)\n",
    "        \n",
    "    return np.array(total_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs = run_x_times(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(np.mean(avgs[:, i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "X_train, X_test, y_train, y_test = shuffle(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "norm_X_train, norm_X_test, _, _ = standardize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "DTR = DecisionTreeRegressor(min_samples_split=5, max_depth=15)\n",
    "DTR.fit(norm_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training RMSE\n",
    "all_RMSE(DTR.predict(norm_X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test total RMSE\n",
    "all_RMSE(DTR.predict(norm_X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Trying variety of different Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DT_errors = []\n",
    "ET_errors = []\n",
    "RF_errors = []\n",
    "\n",
    "def fit_predict_error(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    return RMSE(model.predict(X_test), y_test)\n",
    "\n",
    "for i in range(1, 25):\n",
    "    DTR = DecisionTreeRegressor(max_depth=i)\n",
    "    error = fit_predict_error(DTR, norm_X_train, y_train, norm_X_test, y_test)\n",
    "    DT_errors.append(error.values)\n",
    "    \n",
    "    ETR = ExtraTreesRegressor(max_depth=i)\n",
    "    error = fit_predict_error(ETR, norm_X_train, y_train, norm_X_test, y_test)\n",
    "    ET_errors.append(error.values)\n",
    "    \n",
    "    RFR = RandomForestRegressor(max_depth=i)\n",
    "    error = fit_predict_error(RFR, norm_X_train, y_train, norm_X_test, y_test)\n",
    "    RF_errors.append(error.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting RMSE's for points\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(np.array(DT_errors)[:, 0], label='Decision Tree Error')\n",
    "plt.plot(np.array(ET_errors)[:, 0], label='Extra Trees Error')\n",
    "plt.plot(np.array(RF_errors)[:, 0], label='Random Forest Error')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data up\n",
    "X_train, X_test, y_train, y_test = shuffle(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "norm_X_train, norm_X_test, x_means, x_stds = standardize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y-value skewness\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for col_num in range(len(y_train.columns)):\n",
    "    sns.distplot(y_train.iloc[:, col_num].to_frame(), ax=axes[col_num // 3][col_num % 3]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STANDARDIZE Y-VALUES (this is because y_values are so skewed)\n",
    "norm_y_train, _, y_means, y_stds = standardize(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries for model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nn_arch():\n",
    "    \"\"\"\n",
    "    Make basic neural network architecture for our dataset.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(35, activation='tanh', input_shape=(28,)))\n",
    "    model.add(Dense(15, activation='tanh', input_shape=(35,)))\n",
    "    model.add(Dense(1, activation='tanh', input_shape=(15,)))\n",
    "    # For a mean squared error regression problem\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(\n",
    "            lr=0.001,\n",
    "            momentum=0.9,\n",
    "            nesterov=True\n",
    "        ), #How to Learn\n",
    "        loss='mse', # What to Learn\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit Neural Network\n",
    "model = make_nn_arch()\n",
    "model.fit(\n",
    "    x=norm_X_train,\n",
    "    y=norm_y_train['NBA_points'], \n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def RMSE(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the root mean squared error of the model's predictions.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((y - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(norm_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE for points\n",
    "RMSE(((y_pred * y_stds['NBA_points']) + y_means['NBA_points']).flatten(), y_test['NBA_points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Make Neural Networks for all stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_and_predict():\n",
    "    \n",
    "    rmses = []\n",
    "    \n",
    "    for column in y_train.columns:\n",
    "\n",
    "        # Create and fit Neural Network\n",
    "        model = make_nn_arch()\n",
    "        model.fit(\n",
    "            x=norm_X_train,\n",
    "            y=norm_y_train[column], \n",
    "            epochs=100,\n",
    "        )\n",
    "        y_pred = model.predict(norm_X_test)\n",
    "\n",
    "        # RMSE by stat category\n",
    "        rmse = RMSE(((y_pred * y_stds[column]) + y_means[column]).flatten(), y_test[column])\n",
    "        rmses.append(rmse)\n",
    "    \n",
    "    return rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rmses = train_all_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rmses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Training the final models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "### FINAL MODELS ###\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your best models here! Remove any comments if you don't need them.\n",
    "\n",
    "# Standardize data\n",
    "\n",
    "# Split data\n",
    "\n",
    "# Train models\n",
    "\n",
    "# Predict\n",
    "\n",
    "# Calculate RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def get_predicts(name):\n",
    "    \n",
    "    ### Let this method take in a name, and output predictions for this player!\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    while True:\n",
    "        name = input()\n",
    "        if name == 'exit' or name == 'quit':\n",
    "            break\n",
    "        print(get_predicts(name))\n",
    "# name = 'Stephen Curry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_college('Stephen Curry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
